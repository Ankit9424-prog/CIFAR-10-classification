# -*- coding: utf-8 -*-
"""CIFAR10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13pQpYmggQEXTWazLmJJOH42UGQ8Y1HQw

##CIFAR10 classification
"""

#import necessary libraries
import torch
import torch.nn as nn

import torchvision
from torchvision import datasets
from torchvision import transforms
from torchvision.transforms import ToTensor

import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"
device

"""##Import the data"""

train_data = datasets.CIFAR10(
    root="data",
    train=True,
    download=True,
    transform=ToTensor(),
    target_transform=None
)

test_data = datasets.CIFAR10(
    root="data",
    train=False,
    download=True,
    transform=ToTensor(),
    target_transform=None
)

len(train_data), len(test_data)

class_to_idx = train_data.class_to_idx
class_to_idx

class_names = train_data.classes
class_names

"""###Visualize the data

"""

image, label = train_data[0]
print(f"Image shape: {image.shape}")
plt.imshow(image.permute(1, 2, 0))
plt.title(label)

"""##Create dataloader"""

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_data,
                              batch_size = 32,
                              shuffle = True)

test_dataloader = DataLoader(test_data,
                             batch_size=32,
                             shuffle=False)

print(f"Dataloader: {train_dataloader}\nLength: {len(train_dataloader)}")
print(f"Dataloader: {test_dataloader}\nLength: {len(test_dataloader)}")

"""# Task
Define a convolutional neural network (CNN) architecture for image classification, including convolutional, pooling, and fully connected layers.

"""

class ImageClassfication(nn.Module):
  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:
    super().__init__()

    self.conv1 = nn.Sequential(
        nn.Conv2d(in_channels=input_shape,
                  out_channels=hidden_units,
                  kernel_size=3,
                  padding=1,
                  stride=1),
        nn.ReLU(),
        nn.Conv2d(in_channels=hidden_units,
                  out_channels=hidden_units,
                  kernel_size=3,
                  padding=1,
                  stride=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2)
    )

    self.conv2 = nn.Sequential(
        nn.Conv2d(in_channels=hidden_units,
                  out_channels=hidden_units,
                  kernel_size=3,
                  padding=1,
                  stride=1),
        nn.ReLU(),
        nn.Conv2d(in_channels=hidden_units,
                  out_channels=hidden_units,
                  kernel_size=3,
                  padding=1,
                  stride=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2)
    )

    self.classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features=hidden_units*8*8,
                  out_features=output_shape)
    )

  def forward(self, x:torch.Tensor):
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.classifier(x)
    return x

"""##Define loss function and optimizer"""

model = ImageClassfication(input_shape=3,
                            hidden_units=64, # Increased hidden_units
                            output_shape=len(class_names)).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer =  torch.optim.Adam(params=model.parameters(),
                              lr=0.001)

"""##Define the traning function

"""

def train_step(model: torch.nn.Module,
               dataloader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               device: torch.device=device):

  model.train()

  train_loss, train_acc = 0, 0

  for batch, (X, y) in enumerate(dataloader):
    X, y = X.to(device), y.to(device)
    y_pred = model(X)

    loss =  loss_fn(y_pred, y)
    train_loss += loss.item()

    optimizer.zero_grad() # Corrected: added parentheses

    loss.backward()

    optimizer.step()

    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)
    train_acc += (y_pred_class == y).sum().item()/len(y_pred)

  train_loss = train_loss /len(dataloader)
  train_acc = train_acc / len(dataloader)
  return train_loss, train_acc

"""##Define the evaluation function

"""

def test_step(model: torch.nn.Module,
              dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              optimizer: torch.optim.Optimizer,
              device: torch.device = device):

  model.eval()

  test_loss, test_acc = 0, 0

  with torch.inference_mode():
    for batch, (X, y) in enumerate(dataloader):
      X, y = X.to(device), y.to(device)

      test_pred = model(X)

      loss = loss_fn(test_pred, y)
      test_loss += loss.item()

      test_pred_class = torch.argmax(torch.softmax(test_pred, dim=1), dim=1)
      test_acc += (test_pred_class == y).sum().item()/len(test_pred_class)

  test_loss = test_loss / len(dataloader)
  test_acc = test_acc / len(dataloader)
  return test_loss, test_acc

"""##Define the traning loop"""

epochs = 8  #For now i have done 8 epochs, for better accuracy you can increase it.
results = {
    "train_loss": [],
    "train_acc": [],
    "test_loss": [],
    "test_acc": []
}

for epoch in range(epochs):
  train_loss, train_acc = train_step(model=model,
                                     dataloader=train_dataloader,
                                     loss_fn=loss_fn,
                                     optimizer=optimizer,
                                     device=device)
  test_loss, test_acc = test_step(model=model,
                                  dataloader=test_dataloader,
                                  loss_fn=loss_fn,
                                  optimizer=optimizer,
                                  device=device)

  print(f"Epochs: {epoch+1} |"
        f"train_loss: {train_loss:.4f} |"
        f"train_acc: {train_acc:.4f} |"
        f"test_loss: {test_loss:.4f} |"
        f"test_acc: {test_acc:.4f}")

  results["train_loss"].append(train_loss)
  results["train_acc"].append(train_acc)
  results["test_loss"].append(test_loss)
  results["test_acc"].append(test_acc)

"""##Visualize the traning result"""

plt.figure(figsize=(12, 5))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(results['train_loss'], label='Train Loss')
plt.plot(results['test_loss'], label='Test Loss')
plt.title('Loss per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(results['train_acc'], label='Train Accuracy')
plt.plot(results['test_acc'], label='Test Accuracy')
plt.title('Accuracy per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

"""### Making Predictions with the Trained Model


"""

# Get a sample image and label from the test data
img, true_label = test_data[0]

# Put model in eval mode
model.eval()

# Make prediction with inference mode
with torch.inference_mode():
    # Add an extra dimension to the image (batch size of 1)
    # and send it to the target device
    img_to_predict = img.unsqueeze(dim=0).to(device)
    prediction_logits = model(img_to_predict)

# Get prediction probability
prediction_prob = torch.softmax(prediction_logits, dim=1)

# Get prediction label
prediction_label = torch.argmax(prediction_prob, dim=1).item()

# Convert to class name
predicted_class_name = class_names[prediction_label]
true_class_name = class_names[true_label]

# Plot the image and print prediction results
plt.imshow(img.permute(1, 2, 0), interpolation='nearest') # Added interpolation='nearest'
plt.title(f"True: {true_class_name} | Predicted: {predicted_class_name}")
plt.axis('off')
plt.show()

print(f"Predicted class: {predicted_class_name}")
print(f"True class: {true_class_name}")

model.eval()
y_pred_list = []
with torch.inference_mode():
    for X_test, y_test in test_dataloader:
        X_test, y_test = X_test.to(device), y_test.to(device)
        y_logits = model(X_test)
        y_pred = torch.argmax(torch.softmax(y_logits, dim=1), dim=1)
        y_pred_list.append(y_pred.cpu())

y_pred_tensor = torch.cat(y_pred_list)
print(y_pred_tensor)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Collect true labels from test_data
y_true = []
for _, label in test_data:
    y_true.append(label)
y_true_tensor = torch.tensor(y_true)

# y_pred_list is available from the previous cell's execution
y_pred_tensor = torch.cat(y_pred_list)

# Calculate confusion matrix using sklearn
cm = confusion_matrix(y_true_tensor.numpy(), y_pred_tensor.numpy())

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

